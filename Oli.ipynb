{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training and validation images: 3673\n",
      "Number of test images: 3694\n"
     ]
    }
   ],
   "source": [
    "image_path = Path(\"data/Dataset_filtered\")\n",
    "\n",
    "trainval_image_path = image_path / \"TrainVal/color\"\n",
    "test_image_path = image_path / \"Test/color\"\n",
    "\n",
    "trainval_jpg_count = sum(1 for file in os.listdir(trainval_image_path) if file.lower().endswith(\".jpg\"))\n",
    "print(f\"Number of training and validation images: {trainval_jpg_count}\") #3673\n",
    "\n",
    "test_jpg_count = sum(1 for file in os.listdir(test_image_path) if file.lower().endswith(\".jpg\"))\n",
    "print(f\"Number of test images: {test_jpg_count}\") #3694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training and Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def train_val_split(split_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and validation sets with a species-specific split.\n",
    "    For each species (class), split_ratio of the samples will be assigned to validation.\n",
    "    Updates 'train.txt' and 'val.txt' in the 'annotations' directory.\n",
    "\n",
    "    Assumes each line in trainval.txt is formatted as:\n",
    "        image_name species [other_info...]\n",
    "    \n",
    "    Parameters:\n",
    "        split_ratio (float): The fraction of images per species to be used for validation (default: 0.2)\n",
    "    \"\"\"\n",
    "    annotations_dir = Path(\"Data/annotations\")\n",
    "    trainval_txt = annotations_dir / \"trainval.txt\"\n",
    "    train_txt = annotations_dir / \"train.txt\"\n",
    "    val_txt = annotations_dir / \"val.txt\"\n",
    "\n",
    "    # Read the trainval.txt file\n",
    "    if not trainval_txt.exists():\n",
    "        raise FileNotFoundError(f\"trainval.txt not found in {annotations_dir}\")\n",
    "\n",
    "    with open(trainval_txt, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Group lines by species (assuming species is the second token in each line)\n",
    "    species_dict = {}\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split()\n",
    "        if len(tokens) < 2:\n",
    "            continue  # Skip lines that don't have enough tokens\n",
    "        species = tokens[1]\n",
    "        species_dict.setdefault(species, []).append(line)\n",
    "\n",
    "    train_lines = []\n",
    "    val_lines = []\n",
    "    # For each species, select a fraction of samples for validation\n",
    "    for species, species_lines in species_dict.items():\n",
    "        n_samples = len(species_lines)\n",
    "        n_val = int(n_samples * split_ratio)\n",
    "        # If rounding leads to 0 but there is at least one sample, ensure at least one goes to val\n",
    "        if n_val == 0 and n_samples > 0:\n",
    "            n_val = 1\n",
    "        indices = np.arange(n_samples)\n",
    "        val_indices = np.random.choice(indices, size=n_val, replace=False)\n",
    "        for idx, line in enumerate(species_lines):\n",
    "            if idx in val_indices:\n",
    "                val_lines.append(line)\n",
    "            else:\n",
    "                train_lines.append(line)\n",
    "\n",
    "    # Write new train.txt and val.txt files\n",
    "    with open(train_txt, \"w\") as f_train:\n",
    "        f_train.writelines(train_lines)\n",
    "    with open(val_txt, \"w\") as f_val:\n",
    "        f_val.writelines(val_lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "train_val_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images_to_train_val():\n",
    "    \"\"\"\n",
    "    Copies images and labels for training and validation sets based on the annotation files.\n",
    "    \n",
    "    For each line in train.txt and val.txt, the first token (image name) is used to locate:\n",
    "        - the image file: Dataset_filtered/TrainVal/color/<image_name>.jpg\n",
    "        - the mask file: Dataset_filtered/TrainVal/label/<image_name>.png\n",
    "\n",
    "    The files are then copied to:\n",
    "        - Training set: Dataset_filtered/Train/color and Dataset_filtered/Train/label\n",
    "        - Validation set: Dataset_filtered/Val/color and Dataset_filtered/Val/label\n",
    "    \"\"\"\n",
    "    # Define source directories\n",
    "    base_dir = Path(\"Data/Dataset_filtered\")\n",
    "    source_dir = base_dir / \"TrainVal\"\n",
    "    source_color = source_dir / \"color\"\n",
    "    source_label = source_dir / \"label\"\n",
    "    \n",
    "    # Define destination directories for training and validation\n",
    "    train_dir = base_dir / \"Train\"\n",
    "    val_dir = base_dir / \"Val\"\n",
    "    train_color = train_dir / \"color\"\n",
    "    train_label = train_dir / \"label\"\n",
    "    val_color = val_dir / \"color\"\n",
    "    val_label = val_dir / \"label\"\n",
    "    \n",
    "    # Create destination directories if they do not exist\n",
    "    for folder in [train_color, train_label, val_color, val_label]:\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Define the annotations directory and files\n",
    "    annotations_dir = Path(\"Data/annotations\")\n",
    "    train_txt = annotations_dir / \"train.txt\"\n",
    "    val_txt = annotations_dir / \"val.txt\"\n",
    "    \n",
    "    def copy_files(annotation_file, dest_color, dest_label):\n",
    "        if not annotation_file.exists():\n",
    "            raise FileNotFoundError(f\"{annotation_file} not found.\")\n",
    "        with open(annotation_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split()\n",
    "            if not tokens:\n",
    "                continue  # Skip empty lines\n",
    "            image_name = tokens[0]\n",
    "            # Define source file paths\n",
    "            src_image = source_color / f\"{image_name}.jpg\"\n",
    "            src_label = source_label / f\"{image_name}.png\"\n",
    "            # Copy image file if it exists\n",
    "            if src_image.exists():\n",
    "                shutil.copy(str(src_image), str(dest_color / src_image.name))\n",
    "            else:\n",
    "                print(f\"Warning: {src_image} not found.\")\n",
    "            # Copy label file if it exists\n",
    "            if src_label.exists():\n",
    "                shutil.copy(str(src_label), str(dest_label / src_label.name))\n",
    "            else:\n",
    "                print(f\"Warning: {src_label} not found.\")\n",
    "    \n",
    "    # Copy files for the training set\n",
    "    copy_files(train_txt, train_color, train_label)\n",
    "    # Copy files for the validation set\n",
    "    copy_files(val_txt, val_color, val_label)\n",
    "    \n",
    "    # print(\"Image and label copying completed.\")\n",
    "\n",
    "copy_images_to_train_val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing Images to Make Them Consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = os.cpu_count()\n",
    "print(NUM_WORKERS)\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str,\n",
    "    val_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "  \"\"\"Creates training and testing DataLoaders.\n",
    "\n",
    "  Takes in a training directory and testing directory path and turns\n",
    "  them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
    "\n",
    "  Args:\n",
    "    train_dir: Path to training directory.\n",
    "    test_dir: Path to testing directory.\n",
    "    transform: torchvision transforms to perform on training and testing data.\n",
    "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "    num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "    Where class_names is a list of the target classes.\n",
    "    Example usage:\n",
    "      train_dataloader, val_dataloader, test_dataloader, class_names = \\\n",
    "        = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                             test_dir=path/to/test_dir,\n",
    "                             transform=some_transform,\n",
    "                             batch_size=32,\n",
    "                             num_workers=4)\n",
    "  \"\"\"\n",
    "  # Use ImageFolder to create dataset(s)\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  val_data = datasets.ImageFolder(val_dir, transform=transform)\n",
    "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "  # Get class names\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  # Turn images into data loaders\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  val_dataloader = DataLoader(\n",
    "      val_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,  # don't need to shuffle validation data\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False, # don't need to shuffle test data\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  return train_dataloader, val_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
    "        transforms.ToTensor(), # 2. Turn image values to between 0 & 1 \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
    "                            std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainval_output_path = Path(\"data/Dataset_filtered/TrainVal/resized\")\n",
    "# test_output_path = Path(\"data/Dataset_filtered/Test/resized\")\n",
    "\n",
    "\n",
    "# def resize_image(original_path, output_path, resize_height=256, resize_width=256):\n",
    "\n",
    "#     # Create output directory if not exists\n",
    "#     output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # Define the transformation\n",
    "#     resize_transform = transforms.Compose([\n",
    "#         transforms.Resize((resize_height, resize_width)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
    "#         transforms.ToTensor(), # 2. Turn image values to between 0 & 1 \n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
    "#                             std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
    "#     ])\n",
    "\n",
    "#     # Loop through all .jpg images and resize them\n",
    "#     for img_path in original_path.glob(\"*.jpg\"):  # Iterate over .jpg images\n",
    "#         with Image.open(img_path) as img:\n",
    "#             resized_img = resize_transform(img)  # Apply resizing\n",
    "            \n",
    "#             # Convert back to PIL image for saving\n",
    "#             resized_pil = transforms.ToPILImage()(resized_img)\n",
    "\n",
    "#             # Save resized image\n",
    "#             resized_pil.save(output_path / img_path.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
