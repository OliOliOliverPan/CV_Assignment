import os
import re
import json
from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torch.utils.data import DataLoader
from tqdm import tqdm
from PIL import Image
from torchvision import transforms

from custom_dataset import SegmentationAugment, CustomDataset
from unet import UNet
from enhanced_unet import EnhancedUNet

if __name__ == "__main__":
    # base_dir = "/home/s2103701/Dataset_filtered"
    base_dir = "/Users/bin/Desktop/CV_Assignment/Dataset_filtered"

    # ====== Dataset and Dataloader ======
    train_dataset = CustomDataset(
        image_dir=os.path.join(base_dir, "train_randaug", "color"),
        mask_dir=os.path.join(base_dir, "train_randaug", "label"),
    )

    def custom_collate_fn(batch):
        images, masks, img_names, mask_names = zip(*batch)

        images = torch.stack(images)
        return images, list(masks), list(img_names), list(mask_names)

    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)

    val_dataset = CustomDataset(
        image_dir=os.path.join(base_dir, "val_resized", "color"),
        mask_dir=os.path.join(base_dir, "val_resized", "label"),
    )
    val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)

    # ====== Model & Multi-GPU ======
    model = UNet(in_channels=3, out_channels=4)
    # model = EnhancedUNet(in_channels=3, out_channels=4)

    if torch.cuda.device_count() > 1:
        print(f"ğŸ”§ Using {torch.cuda.device_count()} GPUs with DataParallel.")
        model = nn.DataParallel(model)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # ====== Load original sizes ======
    with open(os.path.join(base_dir, "original_sizes.json"), "r") as f:
        original_sizes = json.load(f)

    def resize_multiclass_logits(logits, img_names, original_sizes_dict):
        resized_logits = []
        for i in range(len(logits)):
            name = img_names[i]
            orig_h, orig_w = original_sizes_dict[name]
            resized = F.interpolate(
                logits[i].unsqueeze(0),
                size=(orig_h, orig_w),
                mode="bilinear",
                align_corners=False
            )
            resized_logits.append(resized.squeeze(0))  # shape: (4, H, W)
        return resized_logits
    
    # ====== â­ï¸  Compute class weights for loss balancing  â­ï¸ ======
    # æˆ‘ä»¬åªç»Ÿè®¡ç±»åˆ« 0~3ï¼ˆå¯¹åº”æœ€ç»ˆçš„æ ‡ç­¾ï¼š0:Cat, 1:Dog, 2:Background, 3:Boundaryï¼‰ï¼Œ
    # mappingè§„åˆ™ä¸CustomDatasetä¸­ä¸€è‡´ï¼š
    #   - å°†åŸå§‹maskå…ˆè½¬ä¸º [0,1] float (èƒŒæ™¯=0ï¼Œè¾¹ç•Œ=1)
    #   - ç„¶åè®¾ is_background = (mask==0), is_boundary = (mask==1)
    #   - å¯¹äºéèƒŒæ™¯éè¾¹ç•Œéƒ¨åˆ†ï¼Œè‹¥æ–‡ä»¶åé¦–å­—æ¯ä¸ºå¤§å†™ï¼Œåˆ™ä¸º Cat (0)ï¼Œå¦åˆ™ä¸º Dog (1)
    #   - æœ€åï¼Œå°†èƒŒæ™¯èµ‹å€¼ä¸º 2ï¼Œè¾¹ç•Œèµ‹å€¼ä¸º 3ã€‚
    print("ğŸ“Š Computing class weights for loss balancing...")
    mask_dir = os.path.join(base_dir, "train_resized", "label")
    num_classes = 4  # åªç»Ÿè®¡ç±»åˆ« 0-3
    class_counts = torch.zeros(num_classes)

    for filename in tqdm(sorted(os.listdir(mask_dir))):
        if filename.endswith(".png"):
            # â­ï¸ ä½¿ç”¨ PIL è¯»å– maskï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º numpy æ•°ç»„
            mask = Image.open(os.path.join(mask_dir, filename)).convert("L")
            mask_np = np.array(mask).astype(np.float32) / 255.0  # å½’ä¸€åŒ–åˆ° [0,1]
            # æ ¹æ®æ˜ å°„è§„åˆ™ç”Ÿæˆæœ€ç»ˆ label:
            # åˆå§‹åŒ–è¾“å‡ºï¼ˆé»˜è®¤å€¼0ï¼‰
            mapped = np.zeros_like(mask_np, dtype=np.int64)
            is_background = (mask_np == 0.0)
            is_boundary = (mask_np == 1.0)
            is_catdog = (~is_background) & (~is_boundary)
            # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­åŠ¨ç‰©ç±»åˆ«
            if filename[0].isupper():
                mapped[is_catdog] = 0  # Cat
            else:
                mapped[is_catdog] = 1  # Dog
            mapped[is_background] = 2  # Background
            mapped[is_boundary] = 3    # Boundary
            # ç´¯è®¡å„ç±»åƒç´ æ•°
            for cls in range(num_classes):
                class_counts[cls] += (mapped == cls).sum()
    print("âœ… Class pixel counts (for classes 0-3):", class_counts.tolist())
    
    # ä½¿ç”¨å€’æ•°ä½œä¸ºæƒé‡
    weights = 1.0 / (class_counts + 1e-6)
    weights = weights / weights.sum()  # normalize
    print("ğŸ¯ Class weights:", weights.tolist())
    # æ³¨æ„ï¼šç”±äºæ¨¡å‹é¢„æµ‹åªæœ‰ 4 ç±»ï¼Œå› æ­¤ loss_fn çš„ weight å‚æ•°åº”ä¸ºé•¿åº¦ä¸º4çš„å¼ é‡

    # ====== Training config ======
    NUM_EPOCHS = 500
    PRINT_INTERVAL = 10
    BEST_MODEL_PATH = "/home/s2103701/Model/best_unet_500_epochs_baseline_aug.pth"
    best_val_loss = float("inf")
    patience = 10
    early_stop_counter = 0

    # loss_fn = nn.CrossEntropyLoss(ignore_index = 4)
    # optimizer = optim.Adam(model.parameters(), lr=0.001)
    # â­ï¸ è®¾ç½® CrossEntropyLoss, ä½¿ç”¨ ignore_index=4 æ¥å¿½ç•¥ unknownï¼ˆground truthä¸­çš„å€¼ä¸º4ï¼‰ 
    loss_fn = nn.CrossEntropyLoss(weight=weights.to(device), ignore_index=4)
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # ====== Training loop ======
    for epoch in range(NUM_EPOCHS):
        model.train()
        total_train_loss = 0.0

        for images, masks, img_names, _ in tqdm(train_dataloader, desc=f"Epoch {epoch+1} Training"):
            images = images.to(device)
            masks = [m.to(device) for m in masks]
            optimizer.zero_grad()

            logits = model(images)
            resized_logits = resize_multiclass_logits(logits, img_names, original_sizes)

            total_loss = 0
            for pred, gt in zip(resized_logits, masks):
                pred = pred.unsqueeze(0)
                gt = gt.unsqueeze(0)
                loss = loss_fn(pred, gt)
                total_loss += loss

            total_loss.backward()
            optimizer.step()
            total_train_loss += total_loss.item()

        avg_train_loss = total_train_loss / len(train_dataloader)

        # ====== Validation ======
        model.eval()
        total_val_loss = 0.0

        with torch.no_grad():
            for images, masks, img_names, _ in tqdm(val_dataloader, desc=f"Epoch {epoch+1} Validation"):
                images = images.to(device)
                masks = [m.to(device) for m in masks]

                logits = model(images)
                resized_logits = resize_multiclass_logits(logits, img_names, original_sizes)

                for pred, gt in zip(resized_logits, masks):
                    pred = pred.unsqueeze(0)
                    gt = gt.unsqueeze(0)
                    val_loss = loss_fn(pred, gt)
                    total_val_loss += val_loss.item()

        avg_val_loss = total_val_loss / len(val_dataloader)

        print(f"Epoch [{epoch+1}/{NUM_EPOCHS}] - Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")

        # âœ… 4. Early Stopping åˆ¤æ–­
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            early_stop_counter = 0  # é‡ç½® counter
            torch.save(model.state_dict(), BEST_MODEL_PATH)
            print(f"ğŸ’¾ Saved new best model at epoch {epoch+1} (val loss: {avg_val_loss:.4f})")
        else:
            early_stop_counter += 1
            print(f"â³ No improvement. Early stop counter: {early_stop_counter}/{patience}")

            if early_stop_counter >= patience:
                print(f"ğŸ›‘ Early stopping triggered at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}")
                break

    print("ğŸ‰ Training complete. Best validation loss:", best_val_loss)
