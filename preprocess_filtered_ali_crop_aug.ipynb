{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1742674233437,
     "user": {
      "displayName": "Guoyue Jeong",
      "userId": "09880000638306830958"
     },
     "user_tz": 0
    },
    "id": "AKxNh8g9eXZD"
   },
   "outputs": [],
   "source": [
    "base_dir = \"/Users/bin/Desktop/CV_Assignment/Dataset_filtered\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰€æœ‰å›¾åƒå’Œæ©ç å·²å¤„ç†å®Œæˆå¹¶ä¿å­˜åˆ° train_crop æ–‡ä»¶å¤¹ã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "input_image_dir = os.path.join(base_dir, \"train\", \"color\")\n",
    "input_mask_dir = os.path.join(base_dir, \"train\", \"label\")\n",
    "\n",
    "output_image_dir = os.path.join(base_dir, \"train_crop\", \"color\")\n",
    "output_mask_dir = os.path.join(base_dir, \"train_crop\", \"label\")\n",
    "\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "# å›ºå®šè£å‰ªå‚æ•°\n",
    "crop_size = (324, 324)  # æ¯”å¦‚ crop åçš„å°ºå¯¸\n",
    "crop_prob = 0.35  # æ¯å¼ å›¾ 35% æ¦‚ç‡è¢«è£å‰ª\n",
    "\n",
    "# éå†æ‰€æœ‰å›¾åƒ\n",
    "image_filenames = sorted([f for f in os.listdir(input_image_dir) if f.endswith(\".jpg\")])\n",
    "\n",
    "for img_name in image_filenames:\n",
    "    img_path = os.path.join(input_image_dir, img_name)\n",
    "    mask_name = img_name.replace(\".jpg\", \".png\")\n",
    "    mask_path = os.path.join(input_mask_dir, mask_name)\n",
    "\n",
    "    # æ‰“å¼€å›¾åƒå’Œ mask\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "    # è·å–åŸå›¾å°ºå¯¸\n",
    "    img_width, img_height = image.size\n",
    "    crop_w, crop_h = crop_size\n",
    "\n",
    "    do_crop = random.random() < crop_prob\n",
    "    use_crop = do_crop and img_width >= crop_w and img_height >= crop_h\n",
    "\n",
    "    if use_crop:\n",
    "        # è·å–éšæœºè£å‰ªåŒºåŸŸ\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(image, output_size=crop_size)\n",
    "        image = transforms.functional.crop(image, i, j, h, w)\n",
    "        mask = transforms.functional.crop(mask, i, j, h, w)\n",
    "        # ä¿®æ”¹æ–‡ä»¶åï¼Œæ ‡è¯†ä¸ºè£å‰ªåçš„ç‰ˆæœ¬\n",
    "        new_img_name = os.path.splitext(img_name)[0] + \"_crop.jpg\"\n",
    "        new_mask_name = os.path.splitext(mask_name)[0] + \"_crop.png\"\n",
    "    else:\n",
    "        new_img_name = img_name\n",
    "        new_mask_name = mask_name\n",
    "\n",
    "    out_img_path = os.path.join(output_image_dir, new_img_name)\n",
    "    out_mask_path = os.path.join(output_mask_dir, new_mask_name)\n",
    "\n",
    "    image.save(out_img_path, format=\"JPEG\")\n",
    "    mask.save(out_mask_path, format=\"PNG\")\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰å›¾åƒå’Œæ©ç å·²å¤„ç†å®Œæˆå¹¶ä¿å­˜åˆ° train_crop æ–‡ä»¶å¤¹ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6rP4qmcWJ_"
   },
   "source": [
    "# Resize images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMmA-9MqcWKA"
   },
   "source": [
    "initializeä¸€ä¸ªdictionaryï¼Œå¯¹æ¯ä¸ªimage resizeä¹‹å‰ï¼Œè®°å½•å®ƒå¯¹åº”çš„maskçš„heightå’Œwidthå¹¶å†™å…¥å­—å…¸ï¼Œæœ€åå°†å­—å…¸å†™å…¥ä¸€ä¸ªå«original_sizes.jsonçš„fileé‡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1742674262860,
     "user": {
      "displayName": "Guoyue Jeong",
      "userId": "09880000638306830958"
     },
     "user_tz": 0
    },
    "id": "oVrKiDtMcWKA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processing complete. Original sizes saved to /Users/bin/Desktop/CV_Assignment/Dataset_filtered/original_sizes.json.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Set target dimensions (adjust as needed)\n",
    "TARGET_WIDTH = 256\n",
    "TARGET_HEIGHT = 256\n",
    "target_size = (TARGET_WIDTH, TARGET_HEIGHT)\n",
    "\n",
    "\n",
    "# Create destination directories:\n",
    "# For training, we resize images but copy masks unchanged.\n",
    "resized_train_color_dir = os.path.join(base_dir, \"train_resized\", \"color\")\n",
    "resized_train_label_dir = os.path.join(base_dir, \"train_resized\", \"label\")\n",
    "\n",
    "# For validation, we resize images and copy masks unchanged.\n",
    "resized_val_color_dir = os.path.join(base_dir, \"val_resized\", \"color\")\n",
    "resized_val_label_dir = os.path.join(base_dir, \"val_resized\", \"label\")\n",
    "\n",
    "for d in [resized_train_color_dir, resized_train_label_dir,\n",
    "          resized_val_color_dir, resized_val_label_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Dictionary to store original image sizes before resizing\n",
    "original_sizes = {}\n",
    "\n",
    "def resize_and_save(src_path, dst_path, target_size):\n",
    "    \"\"\"\n",
    "    Resize an image and save it (don't record size here anymore).\n",
    "    \"\"\"\n",
    "    img = cv2.imread(src_path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        print(\"Error reading:\", src_path)\n",
    "        return\n",
    "    resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "    cv2.imwrite(dst_path, resized)\n",
    "\n",
    "def process_data(color_source, label_source, resized_color_dest, resized_label_dest, original_sizes_dict):\n",
    "    \"\"\"\n",
    "    Processes a dataset split:\n",
    "    - Resizes color images and saves them.\n",
    "    - Copies label masks unchanged, and stores their original sizes with clean keys.\n",
    "    \"\"\"\n",
    "    # Resize color images\n",
    "    for filename in sorted(os.listdir(color_source)):\n",
    "        if filename.lower().endswith(\".jpg\"):\n",
    "            src_path = os.path.join(color_source, filename)\n",
    "            dst_path = os.path.join(resized_color_dest, filename)\n",
    "            resize_and_save(src_path, dst_path, target_size)\n",
    "\n",
    "    # Copy masks and record original sizes\n",
    "    for filename in sorted(os.listdir(label_source)):\n",
    "        if filename.lower().endswith(\".png\"):\n",
    "            src_path = os.path.join(label_source, filename)\n",
    "            dst_path = os.path.join(resized_label_dest, filename)\n",
    "\n",
    "            # Load mask\n",
    "            mask = cv2.imread(src_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if mask is None:\n",
    "                print(\"Error reading mask:\", src_path)\n",
    "                continue\n",
    "\n",
    "            # ğŸ”¥ Clean filename key: remove suffix like \".png\"\n",
    "            img_key = os.path.splitext(filename)[0]  # \"Abyssinian_1.png\" â†’ \"Abyssinian_1\"\n",
    "            original_sizes_dict[img_key] = list(mask.shape)  # Ensure JSON serializable: [H, W]\n",
    "\n",
    "            # Copy mask as-is\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Process Training Data\n",
    "# -----------------------------------------\n",
    "train_color_source = os.path.join(base_dir, \"train_crop\", \"color\")\n",
    "train_label_source = os.path.join(base_dir, \"train_crop\", \"label\")\n",
    "\n",
    "process_data(train_color_source, train_label_source, resized_train_color_dir, resized_train_label_dir, original_sizes)\n",
    "\n",
    "# # -----------------------------------------\n",
    "# # Process Validation Data\n",
    "# # -----------------------------------------\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "val_color_source = os.path.join(val_dir, \"color\")\n",
    "val_label_source = os.path.join(val_dir, \"label\")\n",
    "\n",
    "process_data(val_color_source, val_label_source, resized_val_color_dir, resized_val_label_dir, original_sizes)\n",
    "\n",
    "# Save original sizes to a JSON file\n",
    "original_size_json_path = os.path.join(base_dir, \"original_sizes.json\")\n",
    "with open(original_size_json_path, \"w\") as f:\n",
    "    json.dump(original_sizes, f, indent=4)\n",
    "\n",
    "print(f\"âœ… Processing complete. Original sizes saved to {original_size_json_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EKuuM-NcWKB"
   },
   "source": [
    "å°†imageså’Œmaskséƒ½å†™å…¥DataLoaderé‡Œï¼Œä½†åŒæ—¶ä¹Ÿè¦è®°å½•æ¯å¼ imageçš„nameä»¥ä¾¿åé¢æŸ¥è¯¢maskçš„original size\n",
    "é™¤æ­¤ä»¥å¤–ï¼Œå¯¹äºmask tensoråŒ–çš„æ“ä½œè¿˜è¦æ›´æ”¹ã€‚æˆ‘ä»¬ç›®å‰æœ‰5ä¸ªclass:[cat, dog, background, boundary, unknown] åˆ†åˆ«å¯¹åº”class 0,1,2,3,4,æˆ‘ä»¬é€šè¿‡é»˜è®¤çš„transforms.ToTensor()ä½¿å¾—maskçš„æ¯ä¸ªtensorçš„æ¯ä¸ªåƒç´ éƒ½å˜ä¸ºè¯¥åƒç´ çš„å€¼åï¼Œæˆ‘ä»¬è¿˜è¦è¿›ä¸€æ­¥æ“ä½œ:å¯¹äºæ¯ä¸ªtensorå€¼ï¼Œå¦‚æœæ˜¯0ï¼Œå°±æŠŠè¯¥å€¼å˜ä¸º2ï¼ˆæ„ä¸ºbackground),å¦‚æœæ˜¯1ï¼Œå°±æŠŠè¿™ä¸ªå€¼å˜ä¸º3ï¼ˆæ„ä¸ºboundary),å¦‚æœæ˜¯ä»‹äº0-1ä¹‹é—´çš„å€¼ï¼Œæˆ‘ä»¬çœ‹è¿™ä¸ªmaskçš„filenameï¼Œå¦‚æœfilenameçš„ç¬¬ä¸€ä¸ªå­—æ¯æ˜¯å¤§å†™ï¼Œå°†è¯¥å€¼å˜ä¸º0(æ„ä¸ºçŒ«),åä¹‹å¦‚æœæ˜¯å°å†™ï¼Œå°±å˜æˆ1ï¼ˆæ„ä¸ºç‹—),å¦‚æœæ˜¯å›¾ç‰‡å˜å°äº†ï¼Œæœ‰äº›åŒºåŸŸä¸å±äºåŸå§‹å›¾ç‰‡ï¼Œæˆ‘ä»¬æ ‡ä¸º4ï¼ˆæ„ä¸ºunknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC_DYSv0lEhj"
   },
   "source": [
    "treat boundary as a new class: boundary for now. currently we have 5 classes: [cat, dog, boundary, background, unknown]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resize the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Process Test Data\n",
    "# -----------------------------------------\n",
    "\n",
    "# Directories\n",
    "test_dir = os.path.join(base_dir, \"Test\")\n",
    "resized_test_color_dir = os.path.join(base_dir, \"test_resized\", \"color\")\n",
    "resized_test_label_dir = os.path.join(base_dir, \"test_resized\", \"label\")\n",
    "\n",
    "# Create folders\n",
    "os.makedirs(resized_test_color_dir, exist_ok=True)\n",
    "os.makedirs(resized_test_label_dir, exist_ok=True)\n",
    "\n",
    "# Dictionary for test original sizes\n",
    "original_sizes_test = {}\n",
    "\n",
    "# Source folders\n",
    "test_color_source = os.path.join(test_dir, \"color\")\n",
    "test_label_source = os.path.join(test_dir, \"label\")\n",
    "\n",
    "# Process test data\n",
    "process_data(\n",
    "    color_source=test_color_source,\n",
    "    label_source=test_label_source,\n",
    "    resized_color_dest=resized_test_color_dir,\n",
    "    resized_label_dest=resized_test_label_dir,\n",
    "    original_sizes_dict=original_sizes_test\n",
    ")\n",
    "\n",
    "# Save test sizes JSON\n",
    "test_size_json_path = os.path.join(base_dir, \"original_sizes_test.json\")\n",
    "with open(test_size_json_path, \"w\") as f:\n",
    "    json.dump(original_sizes_test, f, indent=4)\n",
    "\n",
    "print(f\"âœ… Test data processed. Test mask sizes saved to {test_size_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex, Dice\n",
    "import os, json\n",
    "from tqdm import tqdm\n",
    "from unet import UNet\n",
    "from custom_dataset import CustomDataset\n",
    "from enhanced_unet import EnhancedUNet\n",
    "\n",
    "# === Config ===\n",
    "NUM_CLASSES = 4\n",
    "CLASS_NAMES = [\"Cat\", \"Dog\", \"Background\", \"Boundary\"]\n",
    "\n",
    "MODEL_PATH = '/Users/bin/Desktop/CV_Assignment/Model/best_unet_100_epochs_baseline.pth'\n",
    "original_sizes_path = os.path.join(base_dir, \"original_sizes_test.json\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load model ===\n",
    "model = EnhancedUNet(in_channels=3, out_channels=NUM_CLASSES).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === Load original sizes ===\n",
    "with open(original_sizes_path, \"r\") as f:\n",
    "    original_sizes = json.load(f)\n",
    "\n",
    "# === Load test dataset ===\n",
    "test_dataset = CustomDataset(\n",
    "    image_dir=os.path.join(base_dir, \"test_resized\", \"color\"),\n",
    "    mask_dir=os.path.join(base_dir, \"test_resized\", \"label\"),\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# === Metrics ===\n",
    "iou_metric = JaccardIndex(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"none\").to(device)\n",
    "dice_metric = Dice(num_classes=NUM_CLASSES, average=\"none\").to(device)\n",
    "\n",
    "# === Evaluation loop ===\n",
    "with torch.no_grad():\n",
    "    for image, mask, img_name, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        mask = mask.squeeze(0).to(device).long()  # (H, W) with values 0~3\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(image)  # (1, 4, 256, 256)\n",
    "\n",
    "        # Resize logits to original size before argmax\n",
    "        orig_h, orig_w = original_sizes[img_name[0]]\n",
    "        logits_resized = F.interpolate(logits, size=(orig_h, orig_w), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Apply argmax AFTER resizing\n",
    "        pred_mask = torch.argmax(logits_resized, dim=1).long()  # (1, H, W)\n",
    "\n",
    "        # Resize GT mask (if needed) just in case\n",
    "        gt_resized = F.interpolate(mask.unsqueeze(0).unsqueeze(0).float(), size=(orig_h, orig_w), mode=\"nearest\").squeeze(0).long()\n",
    "\n",
    "        # Update metrics\n",
    "        iou_metric.update(pred_mask.to(device), gt_resized.to(device))\n",
    "        dice_metric.update(pred_mask.to(device), gt_resized.to(device))\n",
    "\n",
    "# === Compute final results ===\n",
    "iou_scores = iou_metric.compute()\n",
    "dice_scores = dice_metric.compute()\n",
    "\n",
    "# === Print Results ===\n",
    "print(\"\\nğŸ“Š Per-Class Evaluation on Test Set:\")\n",
    "for i in range(NUM_CLASSES):\n",
    "    print(f\"Class {i} ({CLASS_NAMES[i]}): IoU = {iou_scores[i]:.4f}, Dice = {dice_scores[i]:.4f}\")\n",
    "\n",
    "mean_iou = iou_scores.mean()\n",
    "mean_dice = dice_scores.mean()\n",
    "print(f\"\\nâ¡ï¸ Mean IoU of all classes: {mean_iou:.4f}\")\n",
    "print(f\"â¡ï¸ Mean Dice of all classes: {mean_dice:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex#, Dice\n",
    "import os, json\n",
    "from tqdm import tqdm\n",
    "from unet import UNet\n",
    "from custom_dataset import CustomDataset\n",
    "from enhanced_unet import EnhancedUNet\n",
    "\n",
    "# === Config ===\n",
    "NUM_CLASSES = 4\n",
    "# æˆ‘ä»¬åªå…³æ³¨ 0:Cat, 1:Dog, 2:Backgroundï¼Œå¿½ç•¥ 3:Boundaryï¼ˆä¸å‚ä¸æŒ‡æ ‡è®¡ç®—ï¼‰\n",
    "CLASS_NAMES = [\"Cat\", \"Dog\", \"Background\", \"Boundary\"]\n",
    "\n",
    "MODEL_PATH = '/Users/bin/Desktop/CV_Assignment/Model/best_enhanced_unet_100_epochs.pth'\n",
    "original_sizes_path = os.path.join(base_dir, \"original_sizes_test.json\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load model ===\n",
    "model = EnhancedUNet(in_channels=3, out_channels=NUM_CLASSES).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === Load original sizes ===\n",
    "with open(original_sizes_path, \"r\") as f:\n",
    "    original_sizes = json.load(f)\n",
    "\n",
    "# === Load test dataset ===\n",
    "test_dataset = CustomDataset(\n",
    "    image_dir=os.path.join(base_dir, \"test_resized\", \"color\"),\n",
    "    mask_dir=os.path.join(base_dir, \"test_resized\", \"label\"),\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# === Metrics ===\n",
    "# æ³¨æ„ï¼šè¿™é‡Œè®¾ç½® ignore_index=3ï¼Œè¡¨ç¤ºåœ¨è®¡ç®—æ—¶å¿½ç•¥ ground truth ä¸­æ ‡ç­¾ä¸º3çš„åƒç´ \n",
    "iou_metric = JaccardIndex(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"none\", ignore_index=3).to(device)\n",
    "#dice_metric = Dice(num_classes=NUM_CLASSES, average=\"none\", ignore_index=3).to(device)\n",
    "\n",
    "# === Evaluation loop ===\n",
    "with torch.no_grad():\n",
    "    for image, mask, img_name, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        # mask: (H, W) with values 0~3, convert to long tensor\n",
    "        mask = mask.squeeze(0).to(device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(image)  # (1, 4, 256, 256)\n",
    "\n",
    "        # Resize logits to original size before argmax\n",
    "        orig_h, orig_w = original_sizes[img_name[0]]\n",
    "        logits_resized = F.interpolate(logits, size=(orig_h, orig_w), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Apply argmax AFTER resizing\n",
    "        pred_mask = torch.argmax(logits_resized, dim=1).long()  # (1, H, W)\n",
    "\n",
    "        # Resize GT mask to original size if needed\n",
    "        gt_resized = F.interpolate(mask.unsqueeze(0).unsqueeze(0).float(), size=(orig_h, orig_w), mode=\"nearest\").squeeze(0).long()\n",
    "\n",
    "        # Update metrics; pixels where gt == 3 are ignored automatically\n",
    "        iou_metric.update(pred_mask.to(device), gt_resized.to(device))\n",
    "        #dice_metric.update(pred_mask.to(device), gt_resized.to(device))\n",
    "\n",
    "# === Compute final results ===\n",
    "iou_scores = iou_metric.compute()  # è¿”å› shape (num_classes,) ï¼Œå…¶ä¸­ ignore_index=3 ä¸ä¼šè®¡å…¥ç»“æœ\n",
    "#dice_scores = dice_metric.compute()\n",
    "\n",
    "# ä»…è¾“å‡ºç±»åˆ« 0,1,2 çš„æŒ‡æ ‡\n",
    "print(\"\\nğŸ“Š Per-Class Evaluation on Test Set (excluding class 3):\")\n",
    "for i in range(3):\n",
    "    print(f\"Class {i} ({CLASS_NAMES[i]}): IoU = {iou_scores[i]:.4f}\")#, Dice = {dice_scores[i]:.4f}\")\n",
    "\n",
    "# è®¡ç®— mean IoU / Diceï¼Œåªè€ƒè™‘ class 0,1,2\n",
    "mean_iou = iou_scores[:3].mean()\n",
    "# mean_dice = dice_scores[:3].mean()\n",
    "print(f\"\\nâ¡ï¸ Mean IoU of classes 0-2: {mean_iou:.4f}\")\n",
    "# print(f\"â¡ï¸ Mean Dice of classes 0-2: {mean_dice:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
