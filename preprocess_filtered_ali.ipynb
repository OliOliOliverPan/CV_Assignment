{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1742674233437,
     "user": {
      "displayName": "Guoyue Jeong",
      "userId": "09880000638306830958"
     },
     "user_tz": 0
    },
    "id": "AKxNh8g9eXZD"
   },
   "outputs": [],
   "source": [
    "base_dir = \"/Users/bin/Desktop/CV_Assignment/Dataset_filtered\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29367,
     "status": "ok",
     "timestamp": 1742674262762,
     "user": {
      "displayName": "Guoyue Jeong",
      "userId": "09880000638306830958"
     },
     "user_tz": 0
    },
    "id": "d5pNrLzodT_7",
    "outputId": "b7a87a82-a418-452c-d202-651df541def9"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "try:\n",
    "    import torchinfo\n",
    "except:\n",
    "    !pip install torchinfo\n",
    "    import torchinfo\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AijK3ImLcWJ8"
   },
   "source": [
    "# split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1742674262783,
     "user": {
      "displayName": "Guoyue Jeong",
      "userId": "09880000638306830958"
     },
     "user_tz": 0
    },
    "id": "n1ZSBTrFcWJ9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# Set seed for reproducibility\n",
    "# random.seed(42)\n",
    "\n",
    "def trainval_split(base_dir):\n",
    "\n",
    "    # Define base directories\n",
    "    trainval_dir = os.path.join(base_dir, \"TrainVal\")\n",
    "    color_dir = os.path.join(trainval_dir, \"color\")\n",
    "    label_dir = os.path.join(trainval_dir, \"label\")\n",
    "\n",
    "    # Define destination directories for train and validation splits\n",
    "    train_color_dir = os.path.join(base_dir, \"train\", \"color\")\n",
    "    train_label_dir = os.path.join(base_dir, \"train\", \"label\")\n",
    "    val_color_dir = os.path.join(base_dir, \"val\", \"color\")\n",
    "    val_label_dir = os.path.join(base_dir, \"val\", \"label\")\n",
    "\n",
    "    # Create the destination directories if they don't exist\n",
    "    for d in [train_color_dir, train_label_dir, val_color_dir, val_label_dir]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    # Find all jpg files in the color folder\n",
    "    image_files = glob.glob(os.path.join(color_dir, \"*.jpg\"))\n",
    "\n",
    "    # Group files by animal name (assumes format like \"Abyssinian_1.jpg\")\n",
    "    groups = {}\n",
    "    pattern = re.compile(r'(.+)_\\d+\\.jpg')  # capture group for the animal name\n",
    "    for image_file in image_files:\n",
    "        basename = os.path.basename(image_file)\n",
    "        match = pattern.match(basename)\n",
    "        if match:\n",
    "            animal = match.group(1)\n",
    "        else:\n",
    "            animal = \"unknown\"\n",
    "        groups.setdefault(animal, []).append(basename)\n",
    "\n",
    "    # Split the files for each animal into train (80%) and val (20%)\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    for animal, files in groups.items():\n",
    "        random.shuffle(files)\n",
    "        split_index = int(0.8 * len(files))\n",
    "        train_files = files[:split_index]\n",
    "        val_files = files[split_index:]\n",
    "        train_list.extend(train_files)\n",
    "        val_list.extend(val_files)\n",
    "\n",
    "    # Define output text file paths\n",
    "    train_txt_path = os.path.join(base_dir, \"train.txt\")\n",
    "    val_txt_path = os.path.join(base_dir, \"val.txt\")\n",
    "\n",
    "    # Copy files to new folders and write paths to train.txt and val.txt\n",
    "    with open(train_txt_path, \"w\") as train_file, open(val_txt_path, \"w\") as val_file:\n",
    "        # Process train split\n",
    "        for filename in train_list:\n",
    "            # Copy color image\n",
    "            src_color = os.path.join(color_dir, filename)\n",
    "            dst_color = os.path.join(train_color_dir, filename)\n",
    "            shutil.copy2(src_color, dst_color)\n",
    "\n",
    "            # Derive corresponding label filename (change extension from .jpg to .png)\n",
    "            label_filename = filename.replace(\".jpg\", \".png\")\n",
    "            src_label = os.path.join(label_dir, label_filename)\n",
    "            dst_label = os.path.join(train_label_dir, label_filename)\n",
    "            if os.path.exists(src_label):\n",
    "                shutil.copy2(src_label, dst_label)\n",
    "\n",
    "            # Write relative paths to train.txt (format: \"train/color/<filename> train/label/<label_filename>\")\n",
    "            train_file.write(f\"{os.path.join('train','color',filename)} {os.path.join('train','label',label_filename)}\\n\")\n",
    "\n",
    "        # Process validation split\n",
    "        for filename in val_list:\n",
    "            # Copy color image\n",
    "            src_color = os.path.join(color_dir, filename)\n",
    "            dst_color = os.path.join(val_color_dir, filename)\n",
    "            shutil.copy2(src_color, dst_color)\n",
    "\n",
    "            # Corresponding label filename\n",
    "            label_filename = filename.replace(\".jpg\", \".png\")\n",
    "            src_label = os.path.join(label_dir, label_filename)\n",
    "            dst_label = os.path.join(val_label_dir, label_filename)\n",
    "            if os.path.exists(src_label):\n",
    "                shutil.copy2(src_label, dst_label)\n",
    "\n",
    "            # Write relative paths to val.txt\n",
    "            val_file.write(f\"{os.path.join('val','color',filename)} {os.path.join('val','label',label_filename)}\\n\")\n",
    "\n",
    "    print(\"Data splitting and copying complete.\")\n",
    "    print(f\"Train list written to {train_txt_path}\")\n",
    "    print(f\"Val list written to {val_txt_path}\")\n",
    "\n",
    "# trainval_split(base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6rP4qmcWJ_"
   },
   "source": [
    "# Resize images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMmA-9MqcWKA"
   },
   "source": [
    "initialize‰∏Ä‰∏™dictionaryÔºåÂØπÊØè‰∏™image resize‰πãÂâçÔºåËÆ∞ÂΩïÂÆÉÂØπÂ∫îÁöÑmaskÁöÑheightÂíåwidthÂπ∂ÂÜôÂÖ•Â≠óÂÖ∏ÔºåÊúÄÂêéÂ∞ÜÂ≠óÂÖ∏ÂÜôÂÖ•‰∏Ä‰∏™Âè´original_sizes.jsonÁöÑfileÈáå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1742674262860,
     "user": {
      "displayName": "Guoyue Jeong",
      "userId": "09880000638306830958"
     },
     "user_tz": 0
    },
    "id": "oVrKiDtMcWKA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Set target dimensions (adjust as needed)\n",
    "TARGET_WIDTH = 256\n",
    "TARGET_HEIGHT = 256\n",
    "target_size = (TARGET_WIDTH, TARGET_HEIGHT)\n",
    "\n",
    "# Directories for train and val splits\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "\n",
    "# Create destination directories:\n",
    "# For training, we resize images but copy masks unchanged.\n",
    "resized_train_color_dir = os.path.join(base_dir, \"train_resized\", \"color\")\n",
    "resized_train_label_dir = os.path.join(base_dir, \"train_resized\", \"label\")\n",
    "\n",
    "# For validation, we resize images and copy masks unchanged.\n",
    "resized_val_color_dir = os.path.join(base_dir, \"val_resized\", \"color\")\n",
    "resized_val_label_dir = os.path.join(base_dir, \"val_resized\", \"label\")\n",
    "\n",
    "for d in [resized_train_color_dir, resized_train_label_dir,\n",
    "          resized_val_color_dir, resized_val_label_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Dictionary to store original image sizes before resizing\n",
    "original_sizes = {}\n",
    "\n",
    "def resize_and_save(src_path, dst_path, target_size):\n",
    "    \"\"\"\n",
    "    Resize an image and save it (don't record size here anymore).\n",
    "    \"\"\"\n",
    "    img = cv2.imread(src_path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        print(\"Error reading:\", src_path)\n",
    "        return\n",
    "    resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "    cv2.imwrite(dst_path, resized)\n",
    "\n",
    "def process_data(color_source, label_source, resized_color_dest, resized_label_dest, original_sizes_dict):\n",
    "    \"\"\"\n",
    "    Processes a dataset split:\n",
    "    - Resizes color images and saves them.\n",
    "    - Copies label masks unchanged, and stores their original sizes with clean keys.\n",
    "    \"\"\"\n",
    "    # Resize color images\n",
    "    for filename in sorted(os.listdir(color_source)):\n",
    "        if filename.lower().endswith(\".jpg\"):\n",
    "            src_path = os.path.join(color_source, filename)\n",
    "            dst_path = os.path.join(resized_color_dest, filename)\n",
    "            resize_and_save(src_path, dst_path, target_size)\n",
    "\n",
    "    # Copy masks and record original sizes\n",
    "    for filename in sorted(os.listdir(label_source)):\n",
    "        if filename.lower().endswith(\".png\"):\n",
    "            src_path = os.path.join(label_source, filename)\n",
    "            dst_path = os.path.join(resized_label_dest, filename)\n",
    "\n",
    "            # Load mask\n",
    "            mask = cv2.imread(src_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if mask is None:\n",
    "                print(\"Error reading mask:\", src_path)\n",
    "                continue\n",
    "\n",
    "            # üî• Clean filename key: remove suffix like \".png\"\n",
    "            img_key = os.path.splitext(filename)[0]  # \"Abyssinian_1.png\" ‚Üí \"Abyssinian_1\"\n",
    "            original_sizes_dict[img_key] = list(mask.shape)  # Ensure JSON serializable: [H, W]\n",
    "\n",
    "            # Copy mask as-is\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Process Training Data\n",
    "# -----------------------------------------\n",
    "train_color_source = os.path.join(train_dir, \"color\")\n",
    "train_label_source = os.path.join(train_dir, \"label\")\n",
    "\n",
    "process_data(train_color_source, train_label_source, resized_train_color_dir, resized_train_label_dir, original_sizes)\n",
    "\n",
    "# # -----------------------------------------\n",
    "# # Process Validation Data\n",
    "# # -----------------------------------------\n",
    "val_color_source = os.path.join(val_dir, \"color\")\n",
    "val_label_source = os.path.join(val_dir, \"label\")\n",
    "\n",
    "process_data(val_color_source, val_label_source, resized_val_color_dir, resized_val_label_dir, original_sizes)\n",
    "\n",
    "# Save original sizes to a JSON file\n",
    "original_size_json_path = os.path.join(base_dir, \"original_sizes.json\")\n",
    "with open(original_size_json_path, \"w\") as f:\n",
    "    json.dump(original_sizes, f, indent=4)\n",
    "\n",
    "# print(f\"‚úÖ Processing complete. Original sizes saved to {original_size_json_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN2nCJwScWKB"
   },
   "source": [
    "# randaugment (Do the augmentations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1742674262861,
     "user": {
      "displayName": "Guoyue Jeong",
      "userId": "09880000638306830958"
     },
     "user_tz": 0
    },
    "id": "-6bFQ4ohcWKB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import math\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "# Define the RandAugment operation pool (14 ops as in the original paper)\n",
    "RA_OPERATIONS = [\n",
    "    \"Identity\", \"AutoContrast\", \"Equalize\",\n",
    "    \"Rotate\", \"Solarize\", \"Color\", \"Posterize\",\n",
    "    \"Contrast\", \"Brightness\", \"Sharpness\",\n",
    "    \"ShearX\", \"ShearY\", \"TranslateX\", \"TranslateY\"\n",
    "]\n",
    "\n",
    "def apply_operation(img, mask, op_name, magnitude):\n",
    "    \"\"\"\n",
    "    Apply a single augmentation operation to the image (and mask, if applicable).\n",
    "    'magnitude' is on a 0-10 scale indicating severity.\n",
    "    \"\"\"\n",
    "    if op_name == \"Identity\":\n",
    "        return img, mask  # no change\n",
    "\n",
    "    if op_name == \"AutoContrast\":\n",
    "        img = ImageOps.autocontrast(img)\n",
    "        return img, mask\n",
    "\n",
    "    if op_name == \"Equalize\":\n",
    "        img = ImageOps.equalize(img)\n",
    "        return img, mask\n",
    "\n",
    "    if op_name == \"Rotate\":\n",
    "        max_deg = 30.0\n",
    "        angle = magnitude / 10.0 * max_deg\n",
    "        if random.random() < 0.5:\n",
    "            angle = -angle\n",
    "        img = img.rotate(angle, resample=Image.BILINEAR, fillcolor=0)\n",
    "        mask = mask.rotate(angle, resample=Image.NEAREST, fillcolor=0)\n",
    "        return img, mask\n",
    "\n",
    "    if op_name == \"Solarize\":\n",
    "        thresh = int(256 - (magnitude / 10.0) * 256)\n",
    "        img = ImageOps.solarize(img, thresh)\n",
    "        return img, mask\n",
    "\n",
    "    if op_name == \"Color\":\n",
    "        factor = 1.0 + (magnitude / 10.0) * 0.9\n",
    "        if random.random() < 0.5:\n",
    "            factor = 1.0 - (magnitude / 10.0) * 0.9\n",
    "        img = ImageEnhance.Color(img).enhance(factor)\n",
    "        return img, mask\n",
    "\n",
    "    if op_name == \"Posterize\":\n",
    "        bits = int(round(8 - (magnitude / 10.0) * 4))\n",
    "        bits = max(1, bits)\n",
    "        img = ImageOps.posterize(img, bits)\n",
    "        return img, mask\n",
    "\n",
    "    if op_name == \"Contrast\":\n",
    "        factor = 1.0 + (magnitude / 10.0) * 0.9\n",
    "        if random.random() < 0.5:\n",
    "            factor = 1.0 - (magnitude / 10.0) * 0.9\n",
    "        img = ImageEnhance.Contrast(img).enhance(factor)\n",
    "        return img, mask\n",
    "\n",
    "    if op_name == \"Brightness\":\n",
    "        factor = 1.0 + (magnitude / 10.0) * 0.9\n",
    "        if random.random() < 0.5:\n",
    "            factor = 1.0 - (magnitude / 10.0) * 0.9\n",
    "        img = ImageEnhance.Brightness(img).enhance(factor)\n",
    "        return img, mask\n",
    "\n",
    "    if op_name == \"Sharpness\":\n",
    "        factor = 1.0 + (magnitude / 10.0) * 0.9\n",
    "        if random.random() < 0.5:\n",
    "            factor = 1.0 - (magnitude / 10.0) * 0.9\n",
    "        img = ImageEnhance.Sharpness(img).enhance(factor)\n",
    "        return img, mask\n",
    "\n",
    "    # Geometric operations: these need to be applied identically to the mask\n",
    "    if op_name == \"ShearX\":\n",
    "        shear_factor = magnitude / 10.0 * 0.3\n",
    "        if random.random() < 0.5:\n",
    "            shear_factor = -shear_factor\n",
    "        shear_degrees = math.degrees(math.atan(shear_factor))\n",
    "        img = F.affine(img, angle=0.0, translate=(0, 0), scale=1.0,\n",
    "                       shear=(shear_degrees, 0.0),\n",
    "                       interpolation=InterpolationMode.BILINEAR, fill=0)\n",
    "        mask = F.affine(mask, angle=0.0, translate=(0, 0), scale=1.0,\n",
    "                        shear=(shear_degrees, 0.0),\n",
    "                        interpolation=InterpolationMode.NEAREST, fill=0)\n",
    "        return img, mask\n",
    "\n",
    "    if op_name == \"ShearY\":\n",
    "        shear_factor = magnitude / 10.0 * 0.3\n",
    "        if random.random() < 0.5:\n",
    "            shear_factor = -shear_factor\n",
    "        shear_degrees = math.degrees(math.atan(shear_factor))\n",
    "        img = F.affine(img, angle=0.0, translate=(0, 0), scale=1.0,\n",
    "                       shear=(0.0, shear_degrees),\n",
    "                       interpolation=InterpolationMode.BILINEAR, fill=0)\n",
    "        mask = F.affine(mask, angle=0.0, translate=(0, 0), scale=1.0,\n",
    "                        shear=(0.0, shear_degrees),\n",
    "                        interpolation=InterpolationMode.NEAREST, fill=0)\n",
    "        return img, mask\n",
    "\n",
    "    if op_name == \"TranslateX\":\n",
    "        max_frac = 0.45\n",
    "        dx = int(round(magnitude / 10.0 * max_frac * img.width))\n",
    "        if random.random() < 0.5:\n",
    "            dx = -dx\n",
    "        img = F.affine(img, angle=0.0, translate=(dx, 0), scale=1.0,\n",
    "                       shear=(0.0, 0.0), interpolation=InterpolationMode.BILINEAR, fill=0)\n",
    "        mask = F.affine(mask, angle=0.0, translate=(dx, 0), scale=1.0,\n",
    "                        shear=(0.0, 0.0), interpolation=InterpolationMode.NEAREST, fill=0)\n",
    "        return img, mask\n",
    "\n",
    "    if op_name == \"TranslateY\":\n",
    "        max_frac = 0.45\n",
    "        dy = int(round(magnitude / 10.0 * max_frac * img.height))\n",
    "        if random.random() < 0.5:\n",
    "            dy = -dy\n",
    "        img = F.affine(img, angle=0.0, translate=(0, dy), scale=1.0,\n",
    "                       shear=(0.0, 0.0), interpolation=InterpolationMode.BILINEAR, fill=0)\n",
    "        mask = F.affine(mask, angle=0.0, translate=(0, dy), scale=1.0,\n",
    "                        shear=(0.0, 0.0), interpolation=InterpolationMode.NEAREST, fill=0)\n",
    "        return img, mask\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "def randaugment_image_mask(img, mask, N, M):\n",
    "    \"\"\"Apply RandAugment (N operations with magnitude M) to the given PIL image and mask.\"\"\"\n",
    "    ops = random.sample(RA_OPERATIONS, N)  # choose N distinct ops at random\n",
    "    for op in ops:\n",
    "        img, mask = apply_operation(img, mask, op, M)\n",
    "    return img, mask\n",
    "\n",
    "def run_randaugment_train_resized(input_base, output_base, N=2, M=9, num_aug=3):\n",
    "    \"\"\"\n",
    "    Process each image-mask pair from the resized training data.\n",
    "\n",
    "    Expects:\n",
    "      - Images in:  input_base/train_resized/color\n",
    "      - Masks in:   input_base/train_resized/label\n",
    "\n",
    "    Saves augmented outputs in:\n",
    "      - output_base/train_randaugmented/color\n",
    "      - output_base/train_randaugmented/label\n",
    "\n",
    "    Additionally, copies the original image and mask.\n",
    "    Generates 'num_aug' augmented images per original image.\n",
    "    \"\"\"\n",
    "    in_img_dir = os.path.join(input_base, \"train_resized\", \"color\")\n",
    "    in_mask_dir = os.path.join(input_base, \"train_resized\", \"label\")\n",
    "    out_img_dir = os.path.join(output_base, \"train_randaugmented\", \"color\")\n",
    "    out_mask_dir = os.path.join(output_base, \"train_randaugmented\", \"label\")\n",
    "    os.makedirs(out_img_dir, exist_ok=True)\n",
    "    os.makedirs(out_mask_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(in_img_dir):\n",
    "        if not filename.lower().endswith(\".jpg\"):\n",
    "            continue\n",
    "        img_path = os.path.join(in_img_dir, filename)\n",
    "        base_name, ext = os.path.splitext(filename)\n",
    "        # Try common mask extensions (often masks are stored as .png)\n",
    "        mask_candidates = [base_name + \".png\", base_name + \".jpg\", base_name + \".jpeg\", base_name + \".bmp\", base_name + \".tif\"]\n",
    "        mask_path = None\n",
    "        for cand in mask_candidates:\n",
    "            cand_path = os.path.join(in_mask_dir, cand)\n",
    "            if os.path.exists(cand_path):\n",
    "                mask_path = cand_path\n",
    "                break\n",
    "        if mask_path is None:\n",
    "            print(f\"Mask for image {filename} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "        if mask.mode not in [\"L\", \"I\"]:\n",
    "            mask = mask.convert(\"L\")\n",
    "\n",
    "        # Save the original image and mask in the output folders\n",
    "        orig_img_path = os.path.join(out_img_dir, f\"{base_name}_orig.jpg\")\n",
    "        orig_mask_path = os.path.join(out_mask_dir, f\"{base_name}_orig.png\")\n",
    "        img.save(orig_img_path)\n",
    "        mask.save(orig_mask_path)\n",
    "        print(f\"Saved original image to {orig_img_path} and mask to {orig_mask_path}\")\n",
    "\n",
    "        # Generate and save augmented versions\n",
    "        for i in range(num_aug):\n",
    "            aug_img, aug_mask = randaugment_image_mask(img, mask, N, M)\n",
    "            out_img_path = os.path.join(out_img_dir, f\"{base_name}_aug_{i}.jpg\")\n",
    "            out_mask_path = os.path.join(out_mask_dir, f\"{base_name}_aug_{i}.png\")\n",
    "            aug_img.save(out_img_path)\n",
    "            aug_mask.save(out_mask_path)\n",
    "            print(f\"Saved augmented image to {out_img_path} and mask to {out_mask_path}\")\n",
    "\n",
    "# --- Example usage in a notebook cell ---\n",
    "# Set your base directory. In your case, it is:\n",
    "# We'll use the same base directory for output (augmented data will be saved under a new subfolder)\n",
    "# run_randaugment_train_resized(base_dir, base_dir, N=2, M=9, num_aug=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EKuuM-NcWKB"
   },
   "source": [
    "Â∞ÜimagesÂíåmasksÈÉΩÂÜôÂÖ•DataLoaderÈáåÔºå‰ΩÜÂêåÊó∂‰πüË¶ÅËÆ∞ÂΩïÊØèÂº†imageÁöÑname‰ª•‰æøÂêéÈù¢Êü•ËØ¢maskÁöÑoriginal size\n",
    "Èô§Ê≠§‰ª•Â§ñÔºåÂØπ‰∫émask tensorÂåñÁöÑÊìç‰ΩúËøòË¶ÅÊõ¥Êîπ„ÄÇÊàë‰ª¨ÁõÆÂâçÊúâ4‰∏™class:[cat, dog, background, boundary] ÂàÜÂà´ÂØπÂ∫îclass 0,1,2,3,Êàë‰ª¨ÈÄöËøáÈªòËÆ§ÁöÑtransforms.ToTensor()‰ΩøÂæómaskÁöÑÊØè‰∏™tensorÁöÑÊØè‰∏™ÂÉèÁ¥†ÈÉΩÂèò‰∏∫ËØ•ÂÉèÁ¥†ÁöÑÂÄºÂêéÔºåÊàë‰ª¨ËøòË¶ÅËøõ‰∏ÄÊ≠•Êìç‰Ωú:ÂØπ‰∫éÊØè‰∏™tensorÂÄºÔºåÂ¶ÇÊûúÊòØ0ÔºåÂ∞±ÊääËØ•ÂÄºÂèò‰∏∫2ÔºàÊÑè‰∏∫background),Â¶ÇÊûúÊòØ1ÔºåÂ∞±ÊääËøô‰∏™ÂÄºÂèò‰∏∫3ÔºàÊÑè‰∏∫boundary),Â¶ÇÊûúÊòØ‰ªã‰∫é0-1‰πãÈó¥ÁöÑÂÄºÔºåÊàë‰ª¨ÁúãËøô‰∏™maskÁöÑfilenameÔºåÂ¶ÇÊûúfilenameÁöÑÁ¨¨‰∏Ä‰∏™Â≠óÊØçÊòØÂ§ßÂÜôÔºåÂ∞ÜËØ•ÂÄºÂèò‰∏∫0(ÊÑè‰∏∫Áå´),Âèç‰πãÂ¶ÇÊûúÊòØÂ∞èÂÜôÔºåÂ∞±ÂèòÊàê1ÔºàÊÑè‰∏∫Áãó)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38043,
     "status": "ok",
     "timestamp": 1742674300889,
     "user": {
      "displayName": "Guoyue Jeong",
      "userId": "09880000638306830958"
     },
     "user_tz": 0
    },
    "id": "4fDV4BMmcWKB"
   },
   "outputs": [],
   "source": [
    "from drive.MyDrive.CV_Assignment.custom_dataset import CustomDataset\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDataset(\n",
    "    image_dir = os.path.join(base_dir, \"train_randaugmented\", \"color\"),\n",
    "    mask_dir = os.path.join(base_dir, \"train_randaugmented\", \"label\"),\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# Custom collate function to handle filenames\n",
    "def custom_collate_fn(batch):\n",
    "    images, masks, img_names, mask_names = zip(*batch)\n",
    "\n",
    "    img_names = [re.sub(r'(_aug_\\d+|_orig)$', '', name) for name in img_names]\n",
    "    mask_names = [re.sub(r'(_aug_\\d+|_orig)$', '', name) for name in mask_names]\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    # masks = torch.stack(masks)\n",
    "    return images, list(masks), list(img_names), list(mask_names)\n",
    "\n",
    "# Create DataLoader with shuffle=True but keeping filenames\n",
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Test DataLoader\n",
    "# for images, masks, img_names, mask_names in train_dataloader:\n",
    "#     print(\"Batch Image Filenames:\", img_names)\n",
    "#     print(\"Batch Mask Filenames:\", mask_names)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31562,
     "status": "ok",
     "timestamp": 1742674332449,
     "user": {
      "displayName": "Guoyue Jeong",
      "userId": "09880000638306830958"
     },
     "user_tz": 0
    },
    "id": "tvPYU-cKcWKC",
    "outputId": "3db85f9f-9c9d-469d-882e-575bab7c9ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "from drive.MyDrive.CV_Assignment.unet import UNet\n",
    "import torch.optim as optim\n",
    "\n",
    "model = UNet(in_channels=3, out_channels=4)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# summary(model=model,\n",
    "#         input_size=(32, 3, 256, 256), # make sure this is \"input_size\", not \"input_shape\"\n",
    "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "# )\n",
    "\n",
    "x = torch.randn(32, 3, 256, 256)  # Example input tensor\n",
    "output = model(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCZi4DnccWKC",
    "outputId": "0d3860fb-d4d8-4a92-b455-275f230f08c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def resize_multiclass_logits(logits, img_names, original_sizes_dict):\n",
    "    resized_logits = []\n",
    "    for i in range(len(logits)):\n",
    "        name = img_names[i]\n",
    "        orig_h, orig_w = original_sizes_dict[name]\n",
    "        resized = F.interpolate(\n",
    "            logits[i].unsqueeze(0),  # shape (1, 4, 256, 256)\n",
    "            size=(orig_h, orig_w),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        resized_logits.append(resized.squeeze(0))  # shape: (4, H, W)\n",
    "    return resized_logits  # list of tensors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(base_dir, \"original_sizes.json\"), \"r\") as f:\n",
    "    original_sizes_dict = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define number of epochs\n",
    "NUM_EPOCHS = 10\n",
    "PRINT_INTERVAL = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop over multiple epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for images, masks, img_names, mask_names in train_dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = [m.to(device) for m in masks]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1Ô∏è‚É£ Forward pass ‚Äî output shape: (B, 4, 256, 256)\n",
    "        logits = model(images)\n",
    "\n",
    "        # 2Ô∏è‚É£ Resize each predicted logits map to original mask size\n",
    "        resized_logits = resize_multiclass_logits(logits, img_names, original_sizes_dict)  # List of (4, H, W)\n",
    "\n",
    "        # 3Ô∏è‚É£ Compute per-sample loss (cross entropy expects (C, H, W) + (H, W))\n",
    "        total_loss = 0\n",
    "        for pred_logits, gt_mask in zip(resized_logits, masks):\n",
    "          # Add batch dimension to both tensors\n",
    "          pred_logits = pred_logits.unsqueeze(0)  # shape: (1, 4, H, W)\n",
    "          gt_mask = gt_mask.unsqueeze(0)          # shape: (1, H, W)\n",
    "\n",
    "          loss = loss_fn(pred_logits, gt_mask)\n",
    "          total_loss += loss\n",
    "\n",
    "        # 5Ô∏è‚É£ Backward + update\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "\n",
    "    if (epoch + 1) % PRINT_INTERVAL == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] - Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"üéâ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC_DYSv0lEhj"
   },
   "source": [
    "treat boundary as a new class: boundary for now. currently we have 4 classes: [cat, dog, boundary, background],"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
